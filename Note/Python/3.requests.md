### 第一节 requests库的七个主要方法
| requests库的七个主要方法|                                            |
|--------------------|-----------------------------------------------|
| requests.request() | 构造一个人请求，支撑一下各方法的基础方法          |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET          |
| requests.head()    | 获取HTML网页头信息的方法，对应HTTP的HEAD         |
| requests.post()    | 向HTML网页提交POST请求的方法，对应于HTTP的POST   |
| requests.put()     | 向HTML网页提交PUT请求的方法，对应于HTTP的PUT     |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH    |
| requests.delete()  | 向HTML网页提交删除请求，对应于HTTP的DELETE       |

**例子**
```python
>>> import requests                     #导入requests的库
>>> r = requests.get('http://www.baidu.com')
>>> r.status_code                       #值为200表示获取成功，其他值表示失败
200
>>> r.encoding                          #默认的编码方式
'ISO-8859-1'
>>> r.apparent_encoding                 #根据文本内容解析，requests自动猜测的
                                        #编码方式，更接近真实编码方式
'utf-8'
>>> r.encoding = r.apparent_encoding   #切换文本的编码方式
>>> r.text
```


### 第二节 requests库的异常
**网络连接有风险，异常处理很重要。requests库支持六种常见的连接异常**

|异常                       |说明                                     |
|---------------------------|-----------------------------------------|
|requestsConnectionError    |网络连接错误异常，如DNS连接失败，拒绝连接等  |
|requestsHTTPError          |HTTP错误异常                              |
|requestsURLRequired        |URL缺失异常                               |
|requestsTooManyRedirects   |超过最大重定向次数，产生重定向异常          |
|requestsConnectionTimeout  |连接远程服务器超时异常                     |
|requestsURLRequired        |URL缺失异常                               |
|requestsTimeout            |请求URL超时，产生超时异常                  |
|**`r.raise_for_status()`** |**`如果不是200，将产生requestsHTTPError`** |

**例子**
```python
import requests

def getHTMLText(url):
    try:
        kv = {'user-agent':'Mozilla/5.0'} #伪装成浏览器获取信息
        r = requests.get(url,headers = kv,timeout = 30) #超时时间为30s
        r.raise_for_status()              #r.status_code==200，将返回None；否则将引发HTTPError异常
        r.encoding = r.apparent_encoding  #！！！apparent_encoding不要错写为apparent_endcoding,执行可能不会报错，但会产生异常
        return r.text
    except:
        return '产生异常'

if __name__ == '__main__':
    url = 'http://www.baidu.com'
    print(getHTMLText(url))
```

### 第三节 HTTP协议
为了更好地理解与使用request库的方法，需要了解HTTP协议。

- **HTTP协议**

   **HTTP**，Hypertext Transfer  Protocol超文本传输协议。HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。HTTP协议采用URL作为定位网络资源的标志。

- **URL的格式**： `http://host[:port][path]`

    host：合法的Internet主机域名或IP地址。
    port：端口号，缺省端口为80。
    path：请求资源的路径。
    
+ HTTP URL实例：
    + `http://www.bit.edu.cn`
    + `http://220.181.111.188/duty`

HTTP URL的理解：
URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源。

|HTTP协议对资源的操作|                                   |
|--------|-----------------------------------------------|
|**方法**|**说明**                                       |
| GET    |请求获取URL位置的资源                          |
| HEAD   |请求获取URL位置资源的响应消息报告，即获取该资源的头部信息|
| POST   |请求向URL位置的资源后附加新的数据              |
| PUT    |请求向URL位置存储一个资源，覆盖原URL位置的资源 |
| PATCH  |请求局部更新URL位置的资源，即改变该出资源的部分内容|
| DELETE |请求删除URL位置储存的资源                      |

使用HEAD操作例子（使用requests库的方法）

```python

>>> import requests
>>> r = requests.head('http://httpbin.org/get')
>>> r.headers
{'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Origin': '*',
 'Content-Encoding': 'gzip', 'Content-Type': 'application/json', 'Date': 'Mon, 0
9 Dec 2019 05:29:10 GMT', 'Referrer-Policy': 'no-referrer-when-downgrade', 'Serv
er': 'nginx', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'DENY', 'X
-XSS-Protection': '1; mode=block', 'Connection': 'keep-alive'}

```
### 第四节 requests方法详解
requests库的七个主要方法都是以requests.request()为基础封装的。
+ **`requests.request(method,url,**kwargs)`**
    + method:请求方式，对应`GET`,`HEAD`,`POST`等七种。
	    + `r = requests.request('GET',url,**kwargs)`
		+ `r = requests.request('HEAD',url,**kwargs)`
		+ `r = requests.request('POST',url,**kwargs)`
		+ `r = requests.request('PUT',url,**kwargs)`
		+ `r = requests.request('PATCH',url,**kwargs)`
		+ `r = requests.request('delete',url,**kwargs)`
		+ `r = requests.request('OPTIONS',url,**kwargs)`
	+ 拟获取页面的URL链接。
	+ **kwargs控制访问的参数，共13个(均为可选项)。
	    + params：字典或字节序列，作为参数增加到URL中。
		    ```python 
		    #例子（参数'?key1=value1&key2=value2',在访问资源时可以起到筛选数据的作用)：
			>>> kv = {'key1':'value1','key2':'value2'}
			>>> r = requests.request('GET','http://python123.io/ws',params = kv)
			>>> print(r.url)
			https://python123.io/ws?key1=value1&key2=value2
		    ```
		+ data：字典、字节序列或文件对象，作为Request的内容
		    ```python
			>>> kv = {'key1':'value1','key2':'value2'}
			>>> r = requests.request('POST','http://python123.io/ws',data = kv)
			>>> body = 'Hello World!'
			>>> r = requests.request('POST','http://python123.io/ws',data = body)
			```
	    + json：JSON格式的数据，作为Request的内容。
		    ```python
			>>> kv = {'key1':'value1'}
			>>> r = requests.request('POST','http://python123.io/ws',json = kv)
		    ```
		+ headers：字典，HTTP定制头。
		    ```python
			>>> hd = {'user-agent':'Chrome/10'}
			>>> r = requests.request('POST','http://python123.io/ws',headers = hd)
			```
		+ cookies：字典或cookieJar，Request中的cookie。
		+ auth：元组，支持HTTP认证功能。
		+ files:字典类型，传输文件。
		    ```python
			>>> fs = {'file':open('data.xls','rb')}
			>>> r = requests.request('POST','http://python123.io/ws',files = fs)
			```
		+ timeout:设定超时时间，秒为单位。
		    ```python
			#若在限定时间内没访问回数据，将产生timeout异常。
			>>> r = requests.request('GET','http://www.baidu.com',timeout = 10)
			```
		+ proxies：字典类型，设定访问代理服务器，可以增加登录认证。
			```python
			#http代理可增加访问用户名和密码的设置；
			#https代理可修改访问所使用IP的地址，防止对爬虫的逆追踪
			>>> pxs = {'http':'http://user:pass@10.10.10.1:1234
			           'https':'https://user:pass@10.10.10.1:4321'}
			>>> r = requests.request('GET','http://www.baidu.com',proxies = pxs
			```
		+ allow_redirects：True/False,默认为True，重定向开关。
		+ stream：True/False,默认为True，获取内容立即下载开关。
		+ verify：True/False,默认为True，认证SSL证书开关。
		+ cert：本地SSL证书路径。
		
### 第五节 网络爬虫介绍

|**网络爬虫分类** |爬取网页 玩转网页                 |爬取网站 爬取系列网站                 |爬取全网 如谷歌、百度         |
|-----------------|----------------------------------|--------------------------------------|------------------------------|
|**特点**         |小规模，数据量小，爬取速度不敏感  |中规模，数据规模较大爬取速度敏感      |大规模，搜索引擎，爬取速度关键|
|**使用库**       |requests库                        |scrapy库                              |定制开发                      |
|**使用占比**     |>90%                              |-                                     |-                             |

#### 1. 常见的服务器限制爬虫的方法
##### (1)来源审查
+ **判断User-Agent进行限制**：
检查来访的HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。
##### (2)发布公告：Robots协议
+ 告知所有爬虫，网站的爬取策略，要求爬虫遵守。
*发布公告仅起到通知的作用，至于是否遵守，由爬虫自身决定。*
+ Robots协议：
    + **Robots Exclusion Standard**,网络爬虫准则。
    + 作用：网站告知网络爬虫哪些页面可以抓起，哪些不行。
    + 形式：在网站根目录下的robots.txt文件。	
	+ 基本语法:
	    + `*`表示所有，`/`表示根目录
		+ User-agent表示哪些爬虫，例如所有爬虫:`User-agent: *`
		+ Disallow表示不允许
	+ 案例：京东的Robots协议
	    + https://www.jd.com/robots.txt
		    ```
			User-agent: *
			Disallow: /?*
			Disallow: /pop/*.html
			Disallow: /pinpai/*.html?*
			User-agent: EtaoSpider
			Disallow: /
			User-agent:HuihuiSpider
			Disallow: /
			User-agent:GwdangSpider
			Disallow: /
			User-agent:WochachaSpider
			Disallow: /
			```
	+ 其他网站的Robots协议
	    + http://www.baidu.com/robots.txt
		+ http://news.sina.com.cn/robots.txt
		+ http://www.qq.com/robots.txt
		+ http://news.qq.com/robots.txt
		+ http://www.moe.edu.cn/robots.txt (无robots协议)
#### 2. robots协议的使用
##### (1)网络爬虫：自动或人工识别robot.txt，在进行内容爬取。
##### (2)约束性：Robots协议是建议但非约束性，网络爬虫可以不不遵守，但存在法律风险。
##### (3)对Robots的理解

|爬取网页 玩转网页   |爬取网络 爬取系列网站    |爬取全网         |
|--------------------|-------------------------|-----------------|
|访问量很小：可以遵守|非商业且偶尔：建议遵守   |必须遵守         |
|访问量较大：建议遵守|商业利益:必须遵守        |必须遵守         |


#### 3. 爬虫举例


##### (1)使用搜索引擎搜索内容
```python
import requests

def getHTMLText(url):
    keyword = '鱼'
    #keyword = 'hello'#用英文关键词搜索，会显示网络故障。原因未知。
    try:
        hd = {'user-agent':'Mozilla/5.0'} #伪装成浏览器获取信息
        pa = {'wd':keyword}
        r = requests.get(url,headers=hd, params = pa) 
        print(r.raise_for_status())       #r.status_code==200，将返回None；否则将引发HTTPError异常。
        r.encoding = r.apparent_encoding
        return len(r.text)
    except:
        return '产生异常'


if __name__ == '__main__':
    url = 'http://www.baidu.com/s'#百度搜索
    #url = 'http://www.so.com/s'#360搜索
    print(getHTMLText(url))
```
##### （2）使用爬虫下载图片
```python
import requests

import os

url ='https://images.pexels.com/photos/3300825/pexels-photo-3300825.jpeg?cs=srgb&dl=cityscape-3300825.jpg&fm=jpg'
root = 'D://pics//'
path = root + '1.jpg'
#path = root + url.split('/')[-1]#按照图片本来名字命名，但可能含特殊符号会导致创建文件失败
try:
    if not os.path.exists(root):
        os.mkdir(root)
    if not os.path.exists(path):
        r = requests.get(url)
        with open(path,'wb') as f:
            f.write(r.content) #将获取的二进制数据写入文件
            f.close()
            print('文件保存成功')
    else:
        print('文件已存在')
except:
    print('爬取失败')
```
##### （3）查询IP
```python
import requests

def iplookup(ip):
    url = 'http://www.ip138.com/iplookup.asp?ip='
    try:
        r = requests.get(url+ip)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        print(r.text)
    except:
        print('查询IP失败')

if __name__ == '__main__':
    ip = '202.204.80.112'
    iplookup(ip)
```

### 第六节 信息提取——BeautifulSoup库

#### 1. 介绍beautiful sopu库
##### （1）安装beautiful soup库
+ `>>>pip install beautifulsoup4`
或者
+ `>>>pip3 install beautifulsoup4`
##### （2）beautiful soup库的使用文档
+ https://beautifulsoup.readthedocs.io/zh_CN/latest/
##### （3）BeautifulSoup的解析器
|解析器            |使用方法                          |条件                  |
|------------------|----------------------------------|--------------------|
|bs4的HTML解析器   |BeautifulSoup(mk,'html.parser')   |安装bs4库            |
|lxml的HTML解析器  |BeautifulSoup(mk,'`xlml`')        |pip install xlml     |
|lxml的XML解析器   |BeautifulSoup(mk,'xml')           |pip install xlml     |
|html5lib的解析器  |BeautifulSoup(mk,'html5lib')      |pip install html5lib |
##### （4）Beautiful Soup类的基本元素

|基本元素|说明|
|-|-|
|Tag            |标签，最基本的信息组织单元，分别用一和<>标明开头和结尾|
|Name           |标签的名字，`<p>...</p>`的名字是‘p’，格式：`<tag>.name`|
|Attributes     |标签的属性，字典形式组织，格式：`<tag>.attrs`|
|NavigableString|标签内非属性字符串，`<>...</>`中字符串，格式：`<tag>.string`|
|Comment        |标签内字符串的注释部分，一种特殊的Comment类型|


#### 2. 使用beautiful sopu库
##### （1）使用beautiful sopu解析网页
```python

# encoding:utf-8

import requests
from bs4 import BeautifulSoup as bs

r = requests.get('http://python123.io/ws/demo.html')
print(r.text)                                       #获取的原网页
print('\n BeautifulSoup Parsing: \n')               #以下为BeautifulSoup解析结果
soup = bs(r.text,'html.parser')                     #解析后的网页
print(soup.prettify())
```
